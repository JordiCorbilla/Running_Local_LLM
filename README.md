# Running a Local LLM

Running a local LLM on your desktop/server:

## Download LM Studio:

- Go to https://lmstudio.ai/ and download the latest version for Windows:

![image](https://github.com/JordiCorbilla/Running_Local_LLM/assets/7347994/5986f2da-ff44-4a02-9546-1ae866bb39f7)

- Run the setup file (LM-Studio-0.2.12-Setup.exe) to install LM Studio locally.

- Once installed, LM Studio will open:

![image](https://github.com/JordiCorbilla/Running_Local_LLM/assets/7347994/c0f90407-4478-4885-9e01-6b4f06a68367)

## Download LLMs:

- Search for the latest Llama-2 model and install the one that will fit in your machine:

![image](https://github.com/JordiCorbilla/Running_Local_LLM/assets/7347994/01c39544-df6b-4d19-a2b8-4c3a6e0a6269)

- Navigate to the chat section and select the downloaded model and start chatting!:

![image](https://github.com/JordiCorbilla/Running_Local_LLM/assets/7347994/aa45b7b7-9c33-4562-8bea-294230b33f04)

## Enable the local inference server

- We can expose this model so we can access it programmatically:
- Navigate to the local server tab and start the server on port 1234.

![image](https://github.com/JordiCorbilla/Running_Local_LLM/assets/7347994/63c84905-01e4-4a7a-b97c-9dce50f2a564)

##  Use OpenAI API to talk to the model



- 
- 
